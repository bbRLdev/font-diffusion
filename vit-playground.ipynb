{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "from vit_pytorch.efficient import ViT\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf home1/08823/msrodlab/.cache/huggingface/datasets/json\n",
    "dataset = load_dataset(\"json\", data_files={'train': 'train-metadata.jsonl', 'test': 'test-metadata.jsonl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(train_dataset[0]['image'])\n",
    "print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Image as HuggingFaceImage\n",
    "\n",
    "def prepare_dataset_for_vit_training(dataset):\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    train_vit_imgs_only = train_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties'])\n",
    "    train_vit_imgs_only = train_vit_imgs_only.class_encode_column(\"character\")\n",
    "    train_vit_imgs_only = train_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "    train_vit_imgs_only = train_vit_imgs_only.with_format('torch')\n",
    "\n",
    "    test_vit_imgs_only = test_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties'])\n",
    "    test_vit_imgs_only = test_vit_imgs_only.class_encode_column(\"character\")\n",
    "    test_vit_imgs_only = test_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "    test_vit_imgs_only = test_vit_imgs_only.with_format('torch')\n",
    "    return train_vit_imgs_only, test_vit_imgs_only\n",
    "\n",
    "def graph_random_sample(vit_dataset):\n",
    "    _, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    transform = transforms.ToPILImage()\n",
    "    for _, ax in enumerate(axes.ravel()):\n",
    "        r_idx = np.random.randint(len(vit_dataset), size=(1,))\n",
    "        ex = vit_dataset[r_idx]\n",
    "        img_tensor = ex['image']\n",
    "        img_tensor = img_tensor.squeeze(0).permute(2, 0, 1)\n",
    "        # print(ex['image'].squeeze(0).shape)\n",
    "        img = transform(img_tensor)\n",
    "        ax.set_title(ex['character'][0])\n",
    "        ax.imshow(img)\n",
    "def get_dataloaders(train_vit_dataset, valid_vit_dataset, test_vit_dataset, batch_size):\n",
    "    train_loader = DataLoader(dataset=train_vit_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset=valid_vit_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_vit_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "def prepare_batch(batch):\n",
    "    batch_imgs = batch['image']\n",
    "    batch_labels = batch['character']\n",
    "    batch_imgs = batch_imgs.permute(0, 3, 1, 2)\n",
    "    batch_imgs = batch_imgs.type('torch.FloatTensor')\n",
    "    return batch_imgs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files={'train': 'train-metadata.jsonl', 'test': 'test-metadata.jsonl'})\n",
    "train_vit_dataset, test_vit_dataset = prepare_dataset_for_vit_training(dataset)\n",
    "train_vit_dataset = train_vit_dataset.train_test_split(test_size=0.1)\n",
    "batch_size = 8\n",
    "train_loader, valid_loader, test_loader = get_dataloaders(train_vit_dataset['train'], train_vit_dataset['test'], test_vit_dataset, batch_size)\n",
    "efficient_transformer = Linformer(\n",
    "    dim=128,\n",
    "    seq_len=256+1,  # 16x16 patches + 1 cls-token\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    k=64\n",
    ")\n",
    "device = 'cuda'\n",
    "model = ViT(\n",
    "    dim=128,\n",
    "    image_size=512,\n",
    "    patch_size=32,\n",
    "    num_classes=62,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=3,\n",
    ").to(device)\n",
    "# loss function\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(4):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_imgs, batch_labels = prepare_batch(batch)\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        output = model(batch_imgs)\n",
    "        loss = criterion(output, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == batch_labels).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for batch in valid_loader:\n",
    "            batch_imgs, batch_labels = prepare_batch(batch)\n",
    "            batch_imgs = batch_imgs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            val_output = model(batch_imgs)\n",
    "            val_loss = criterion(val_output, batch_labels)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == batch_labels).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #transform pipeline\n",
    "# import albumentations as A\n",
    "# import timm\n",
    "# from datasets import load_dataset\n",
    "# from datasets import Image as HuggingFaceImage\n",
    "# def prepare_dataset_for_vit_training(dataset):\n",
    "#     train_dataset = dataset['train']\n",
    "#     test_dataset = dataset['test']\n",
    "#     train_vit_imgs_only = train_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character'])\n",
    "#     train_vit_imgs_only = train_vit_imgs_only.class_encode_column(\"vit_label\")\n",
    "#     train_vit_imgs_only = train_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "#     train_vit_imgs_only = train_vit_imgs_only.with_format('numpy')\n",
    "\n",
    "\n",
    "#     test_vit_imgs_only = test_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character'])\n",
    "#     test_vit_imgs_only = test_vit_imgs_only.class_encode_column(\"vit_label\")\n",
    "#     test_vit_imgs_only = test_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "#     test_vit_imgs_only = test_vit_imgs_only.with_format('numpy')\n",
    "#     return train_vit_imgs_only, test_vit_imgs_only\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     A.augmentations.geometric.resize.Resize(256,256, interpolation=1, always_apply=True, p=1)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HuggingFaceImage\n",
    "\n",
    "\n",
    "def apply_transform(transform, dataset):\n",
    "    def transform_function(example):\n",
    "        transformed = transform(image=example['image'][:,:,0])\n",
    "        example['image'] = transformed['image']\n",
    "        return example\n",
    "\n",
    "    return dataset.map(transform_function)\n",
    "\n",
    "\n",
    "def prepare_dataset_for_vit_training(dataset):\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "\n",
    "    \n",
    "    # train_dataset = apply_transform(transform, train_dataset)\n",
    "    # test_dataset = apply_transform(transform, test_dataset)\n",
    "\n",
    "    train_vit_imgs_only = train_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character'])\n",
    "    train_vit_imgs_only = train_vit_imgs_only.class_encode_column(\"vit_label\")\n",
    "    train_vit_imgs_only = train_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "    train_vit_imgs_only = train_vit_imgs_only.with_format('numpy')\n",
    "\n",
    "    test_vit_imgs_only = test_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character'])\n",
    "    test_vit_imgs_only = test_vit_imgs_only.class_encode_column(\"vit_label\")\n",
    "    test_vit_imgs_only = test_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "    test_vit_imgs_only = test_vit_imgs_only.with_format('numpy')\n",
    "    return train_vit_imgs_only, test_vit_imgs_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=1000, bias=True)\n",
      "Linear(in_features=256, out_features=62, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import timm\n",
    "model = timm.create_model('mobilevitv2_050', in_chans=1)\n",
    "print(model.get_classifier())\n",
    "model.reset_classifier(62)\n",
    "print(model.get_classifier())\n",
    "# dataset = load_dataset(\"json\", data_files={'train': 'train-metadata.jsonl', 'test': 'test-metadata.jsonl'})\n",
    "# train_dataset, test_dataset = prepare_dataset_for_vit_training(dataset)\n",
    "# ex = train_dataset[0]\n",
    "# ex_transform = transform(image=ex['image'])\n",
    "# # print(ex_transform)\n",
    "# img_t = ex_transform['image'][:,:,0]\n",
    "# # out = model(ex.byte())\n",
    "# plt.imshow(img_t)\n",
    "# img_t = torch.tensor(img_t).unsqueeze(-1).permute(2, 0, 1).unsqueeze(0) / 255\n",
    "# print(img_t)\n",
    "# out = model(img_t)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     A.RandomBrightnessContrast(brightness_limit=1, contrast_limit=1, p=1.0),\n",
    "# ])\n",
    "for i,img in enumerate(imgs):\n",
    "    imgs[i]= transform(image=img)['image']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
