{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "from vit_pytorch.efficient import ViT\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Michael Labarca/.cache/huggingface/datasets/json/default-8adb116d29ab3f93/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85d09496abe49b1851b3df5045e0a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !rm -rf home1/08823/msrodlab/.cache/huggingface/datasets/json\n",
    "dataset = load_dataset(\"json\", data_files={'train': 'train-metadata.jsonl', 'test': 'test-metadata.jsonl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['uniqueId', 'image', 'ttf_path', 'font_characteristics', 'character', 'font_properties'],\n",
      "    num_rows: 12028\n",
      "})\n",
      "Dataset({\n",
      "    features: ['uniqueId', 'image', 'ttf_path', 'font_characteristics', 'character', 'font_properties'],\n",
      "    num_rows: 814\n",
      "})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x2DD05BF2880>\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(train_dataset[0]['image'])\n",
    "print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Image as HuggingFaceImage\n",
    "\n",
    "def prepare_dataset_for_vit_training(dataset):\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    train_vit_imgs_only = train_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties'])\n",
    "    train_vit_imgs_only = train_vit_imgs_only.class_encode_column(\"character\")\n",
    "    train_vit_imgs_only = train_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "    train_vit_imgs_only = train_vit_imgs_only.with_format('torch')\n",
    "\n",
    "    test_vit_imgs_only = test_dataset.remove_columns(['uniqueId', 'ttf_path', 'font_characteristics', 'font_properties'])\n",
    "    test_vit_imgs_only = test_vit_imgs_only.class_encode_column(\"character\")\n",
    "    test_vit_imgs_only = test_vit_imgs_only.cast_column('image', HuggingFaceImage())\n",
    "    test_vit_imgs_only = test_vit_imgs_only.with_format('torch')\n",
    "    return train_vit_imgs_only, test_vit_imgs_only\n",
    "\n",
    "def graph_random_sample(vit_dataset):\n",
    "    _, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    transform = transforms.ToPILImage()\n",
    "    for _, ax in enumerate(axes.ravel()):\n",
    "        r_idx = np.random.randint(len(vit_dataset), size=(1,))\n",
    "        ex = vit_dataset[r_idx]\n",
    "        img_tensor = ex['image']\n",
    "        img_tensor = img_tensor.squeeze(0).permute(2, 0, 1)\n",
    "        # print(ex['image'].squeeze(0).shape)\n",
    "        img = transform(img_tensor)\n",
    "        ax.set_title(ex['character'][0])\n",
    "        ax.imshow(img)\n",
    "def get_dataloaders(train_vit_dataset, test_vit_dataset, batch_size):\n",
    "    train_loader = DataLoader(dataset=train_vit_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_vit_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "def prepare_batch(batch):\n",
    "    batch_imgs = batch['image']\n",
    "    batch_labels = batch['character']\n",
    "    batch_imgs = batch_imgs.permute(0, 3, 1, 2)\n",
    "    batch_imgs = batch_imgs.type('torch.FloatTensor')\n",
    "    return batch_imgs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Michael Labarca/.cache/huggingface/datasets/json/default-8adb116d29ab3f93/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee604f654a54ec18a720fec96ebca48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-8adb116d29ab3f93\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-caad88676c897986.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-8adb116d29ab3f93\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-0d83d1eafa0bc688.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68514965cf549438e31047ad1e01404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m batch_imgs, batch_labels \u001b[39m=\u001b[39m prepare_batch(batch)\n\u001b[0;32m     41\u001b[0m \u001b[39mprint\u001b[39m(batch_imgs\u001b[39m.\u001b[39mdtype, batch_labels\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m---> 42\u001b[0m batch_imgs \u001b[39m=\u001b[39m batch_imgs\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     43\u001b[0m batch_labels \u001b[39m=\u001b[39m batch_labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     44\u001b[0m output \u001b[39m=\u001b[39m model(batch_imgs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files={'train': 'train-metadata.jsonl', 'test': 'test-metadata.jsonl'})\n",
    "train_vit_dataset, test_vit_dataset = prepare_dataset_for_vit_training(dataset)\n",
    "batch_size = 8\n",
    "train_loader, test_loader = get_dataloaders(train_vit_dataset, test_vit_dataset, batch_size)\n",
    "efficient_transformer = Linformer(\n",
    "    dim=128,\n",
    "    seq_len=256+1,  # 16x16 patches + 1 cls-token\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    k=64\n",
    ")\n",
    "device = 'cuda'\n",
    "model = ViT(\n",
    "    dim=128,\n",
    "    image_size=512,\n",
    "    patch_size=32,\n",
    "    num_classes=62,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=3,\n",
    ").to(device)\n",
    "# loss function\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(4):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_imgs, batch_labels = prepare_batch(batch)\n",
    "        print(batch_imgs.dtype, batch_labels.dtype)\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        output = model(batch_imgs)\n",
    "        loss = criterion(output, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == batch_labels).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     epoch_val_accuracy = 0\n",
    "    #     epoch_val_loss = 0\n",
    "    #     for data, label in valid_loader:\n",
    "    #         data = data.to(device)\n",
    "    #         label = label.to(device)\n",
    "\n",
    "    #         val_output = model(data)\n",
    "    #         val_loss = criterion(val_output, label)\n",
    "\n",
    "    #         acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "    #         epoch_val_accuracy += acc / len(valid_loader)\n",
    "    #         epoch_val_loss += val_loss / len(valid_loader)\n",
    "\n",
    "    # print(\n",
    "    #     f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    # )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
