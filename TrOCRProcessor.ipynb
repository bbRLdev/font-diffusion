{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HuggingFaceImage\n",
    "from linformer import Linformer\n",
    "from vit_pytorch.efficient import ViT\n",
    "import torch\n",
    "def get_tokenizer() -> CLIPTokenizer:\n",
    "    return CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "def prepare_data(tokenizer: CLIPTokenizer):\n",
    "    def add_prompt(example):\n",
    "        props, key = example['font_properties']\n",
    "        character = example['character']\n",
    "        split = character.split('_')\n",
    "        if len(split) > 1:\n",
    "            character = split[0] + 'case ' + split[1]\n",
    "        else:\n",
    "            character = split[0]\n",
    "        prompt = f\"a {example['font_serifs']} {character} with {props} {key}\" \n",
    "        example['prompt'] = prompt\n",
    "        return example\n",
    "    def map_tokens(example):\n",
    "        prompt = example['prompt']\n",
    "        tokens = tokenizer.encode(prompt, padding='max_length', max_length=42)\n",
    "        example['tokens'] = tokens\n",
    "        return example\n",
    "    dataset = load_dataset('json', data_files={'train':'train-metadata.jsonl', 'test':'test-metadata.jsonl'})\n",
    "    \n",
    "    train_new_column = ['foo'] * len(dataset['train'])\n",
    "    dataset['train'] = dataset['train'].add_column('prompt', train_new_column)\n",
    "    dataset['train'] = dataset['train'].add_column('tokens', train_new_column)\n",
    "    dataset['train'] = dataset['train'].map(add_prompt)\n",
    "    dataset['train'] = dataset['train'].map(map_tokens)\n",
    "    dataset['train'] = dataset['train'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['train'] = dataset['train'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['train'] = dataset['train'].with_format('torch')\n",
    "    \n",
    "    test_new_column = ['bar'] * len(dataset['test'])\n",
    "    dataset['test'] = dataset['test'].add_column('prompt', test_new_column)\n",
    "    dataset['test'] = dataset['test'].add_column('tokens', test_new_column)\n",
    "    dataset['test'] = dataset['test'].map(add_prompt)\n",
    "    dataset['test'] = dataset['test'].map(map_tokens)\n",
    "    dataset['test'] = dataset['test'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['test'] = dataset['test'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['test'] = dataset['test'].with_format('torch')\n",
    "    return dataset\n",
    "def get_vit_model(image_size: int, patch_size: int, dim: int, depth: int, num_heads: int, k: int, device: str):\n",
    "    sequence_length = (image_size//patch_size)**2 + 1\n",
    "    # for 512x512px image with 32x32px patches: 16x16 + 1 CLS token\n",
    "    efficient_transformer = Linformer(\n",
    "        dim=dim,\n",
    "        seq_len=sequence_length,  \n",
    "        depth=depth,\n",
    "        heads=num_heads,\n",
    "        k=k\n",
    "    )\n",
    "    model = ViT(\n",
    "        dim=dim,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_classes=62,\n",
    "        transformer=efficient_transformer,\n",
    "        channels=1,\n",
    "    )\n",
    "    return model \n",
    "def get_vit(image_size, patch_size, vit_dim, vit_depth, vit_num_heads, k, device, vit_checkpoint_path):\n",
    "    vit = get_vit_model(image_size=image_size, \n",
    "                        patch_size=patch_size, \n",
    "                        dim=vit_dim, \n",
    "                        depth=vit_depth, \n",
    "                        num_heads=vit_num_heads, \n",
    "                        k=k, \n",
    "                        device=device)\n",
    "    if vit_checkpoint_path != None:\n",
    "        vit_checkpoint = torch.load(vit_checkpoint_path)\n",
    "        vit.load_state_dict(vit_checkpoint['model_state_dict'])\n",
    "        print('Loaded ViT model from checkpoint:', vit_checkpoint_path)\n",
    "    return vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
    "\n",
    "# Define image transformation\n",
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# # Load pretrained CLIP model and processor\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "#microsoft/trocr-large-handwritten works \n",
    "processor = TrOCRProcessor.from_pretrained(\"anaghasavit/trocr-processor\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_clip_train import prepare_batch, get_dataloaders, get_tokenizer, prepare_data\n",
    "clip_tokenizer = get_tokenizer(True)\n",
    "dataset = prepare_data(clip_tokenizer)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "train_loader, valid_loader = get_dataloaders(train_dataset, test_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Inference function\n",
    "def generate_caption(image_pil, processor, model, top_k=1):\n",
    "\n",
    "    if not isinstance(image_pil, list):\n",
    "        image_pil = [image_pil]\n",
    "\n",
    "    inputs = processor(images=image_pil, return_tensors=\"pt\", padding=True, max_length=77)\n",
    "    \n",
    "    # Create dummy input_ids tensor of zeros with the same batch size as the pixel_values\n",
    "    batch_size = inputs[\"pixel_values\"].shape[0]\n",
    "    input_ids_dummy = torch.zeros((batch_size, 1), dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids_dummy, **inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=-1)\n",
    "        \n",
    "    top_probs, top_indices = torch.topk(probs, k=top_k, dim=-1)\n",
    "    captions = [processor.decode(idx) for idx in top_indices.squeeze().tolist()]\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Iterate over the dataset\n",
    "for batch in valid_loader:\n",
    "    batch_imgs, batch_tokens = prepare_batch(batch)\n",
    "    batch_imgs, batch_tokens = batch_imgs.to('cuda'), batch_tokens.to('cuda')\n",
    "    for i in range(batch_imgs.shape[0]):\n",
    "        image_tensor = batch_imgs[i].cpu()\n",
    "        # rescaled_image_tensor = (image_tensor - image_tensor.min()) / (image_tensor.max() - image_tensor.min())  # Rescale to [0, 1]\n",
    "        grid = make_grid(image_tensor, nrow=1, normalize=True, scale_each=True, padding=0)\n",
    "        image_pil = TF.to_pil_image(grid)\n",
    "        inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        print(outputs)\n",
    "        caption = processor.decode(outputs.squeeze(), skip_special_tokens=True)\n",
    "        print(\"Generated Caption:\", caption)\n",
    "        display(image_pil)\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
