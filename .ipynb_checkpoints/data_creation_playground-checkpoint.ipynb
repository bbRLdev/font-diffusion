{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/08823/msrodlab/maverick2/font-diffusion\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from -r ./requirements.txt (line 1)) (1.21.6)\n",
      "Requirement already satisfied: fontpreview in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from -r ./requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: pandas in /opt/apps/intel18/python3/3.7.0/lib/python3.7/site-packages (from -r ./requirements.txt (line 3)) (0.24.1)\n",
      "Requirement already satisfied: requests in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from -r ./requirements.txt (line 4)) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from -r ./requirements.txt (line 5)) (4.64.1)\n",
      "Requirement already satisfied: vit-pytorch in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from -r ./requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: uuid in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from -r ./requirements.txt (line 7)) (1.30)\n",
      "Requirement already satisfied: Pillow in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from fontpreview->-r ./requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/apps/intel18/python3/3.7.0/lib/python3.7/site-packages (from pandas->-r ./requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/apps/intel18/python3/3.7.0/lib/python3.7/site-packages (from pandas->-r ./requirements.txt (line 3)) (2018.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/apps/intel18/python3/3.7.0/lib/python3.7/site-packages (from requests->-r ./requirements.txt (line 4)) (2018.11.29)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from requests->-r ./requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from requests->-r ./requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from requests->-r ./requirements.txt (line 4)) (1.26.12)\n",
      "Requirement already satisfied: torchvision in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from vit-pytorch->-r ./requirements.txt (line 6)) (0.14.1)\n",
      "Requirement already satisfied: torch>=1.10 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from vit-pytorch->-r ./requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: einops>=0.6.0 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from vit-pytorch->-r ./requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/apps/intel18/python3/3.7.0/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas->-r ./requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from torch>=1.10->vit-pytorch->-r ./requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from torch>=1.10->vit-pytorch->-r ./requirements.txt (line 6)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from torch>=1.10->vit-pytorch->-r ./requirements.txt (line 6)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from torch>=1.10->vit-pytorch->-r ./requirements.txt (line 6)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from torch>=1.10->vit-pytorch->-r ./requirements.txt (line 6)) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/apps/intel18/python3/3.7.0/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->vit-pytorch->-r ./requirements.txt (line 6)) (41.0.1)\n",
      "Requirement already satisfied: wheel in /home1/08823/msrodlab/.local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->vit-pytorch->-r ./requirements.txt (line 6)) (0.38.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO: Name of the dataset usually matches the script name with CamelCase instead of snake_case\n",
    "class NewDataset(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"TODO: Short description of my dataset.\"\"\"\n",
    "\n",
    "    VERSION = datasets.Version(\"1.1.0\")\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "    # If you need to make complex sub-parts in the datasets with configurable options\n",
    "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "    # You will be able to load one or the other configurations in the following list with\n",
    "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "    BUILDER_CONFIGS = [\n",
    "        datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),\n",
    "        datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),\n",
    "    ]\n",
    "\n",
    "    DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
    "\n",
    "    def _info(self):\n",
    "        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\n",
    "        if self.config.name == \"first_domain\":  # This is the name of the configuration selected in BUILDER_CONFIGS above\n",
    "            features = datasets.Features(\n",
    "                {\n",
    "                    \"sentence\": datasets.Value(\"string\"),\n",
    "                    \"option1\": datasets.Value(\"string\"),\n",
    "                    \"answer\": datasets.Value(\"string\")\n",
    "                    # These are the features of your dataset like images, labels ...\n",
    "                }\n",
    "            )\n",
    "        else:  # This is an example to show how to have different features for \"first_domain\" and \"second_domain\"\n",
    "            features = datasets.Features(\n",
    "                {\n",
    "                    \"sentence\": datasets.Value(\"string\"),\n",
    "                    \"option2\": datasets.Value(\"string\"),\n",
    "                    \"second_domain_answer\": datasets.Value(\"string\")\n",
    "                    # These are the features of your dataset like images, labels ...\n",
    "                }\n",
    "            )\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=_DESCRIPTION,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,  # Here we define them above because they are different between the two configurations\n",
    "            # If there's a common (input, target) tuple from the features, uncomment supervised_keys line below and\n",
    "            # specify them. They'll be used if as_supervised=True in builder.as_dataset.\n",
    "            # supervised_keys=(\"sentence\", \"label\"),\n",
    "            # Homepage of the dataset for documentation\n",
    "            homepage=_HOMEPAGE,\n",
    "            # License for the dataset if available\n",
    "            license=_LICENSE,\n",
    "            # Citation for the dataset\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
    "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\n",
    "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
    "        urls = _URLS[self.config.name]\n",
    "        data_dir = dl_manager.download_and_extract(urls)\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, \"train.jsonl\"),\n",
    "                    \"split\": \"train\",\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.VALIDATION,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, \"dev.jsonl\"),\n",
    "                    \"split\": \"dev\",\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, \"test.jsonl\"),\n",
    "                    \"split\": \"test\"\n",
    "                },\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "    def _generate_examples(self, filepath, split):\n",
    "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            for key, row in enumerate(f):\n",
    "                data = json.loads(row)\n",
    "                if self.config.name == \"first_domain\":\n",
    "                    # Yields examples as (key, example) tuples\n",
    "                    yield key, {\n",
    "                        \"sentence\": data[\"sentence\"],\n",
    "                        \"option1\": data[\"option1\"],\n",
    "                        \"answer\": \"\" if split == \"test\" else data[\"answer\"],\n",
    "                    }\n",
    "                else:\n",
    "                    yield key, {\n",
    "                        \"sentence\": data[\"sentence\"],\n",
    "                        \"option2\": data[\"option2\"],\n",
    "                        \"second_domain_answer\": \"\" if split == \"test\" else data[\"second_domain_answer\"],\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation. Run during first time setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yI-JEVVPUxhQ",
    "outputId": "548bbe3e-42b9-4a07-f874-d57592a44785"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import csv\n",
    "from fontpreview import FontPreview\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "import shutil\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, Image\n",
    "from fontTools.ttLib import TTFont\n",
    "from fontTools.unicode import Unicode\n",
    "\n",
    "\n",
    "def has_glyph(font, glyph):\n",
    "    for table in font['cmap'].tables:\n",
    "        if ord(glyph) in table.cmap.keys():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#For a given font file, create the alphabet and the numbers 0-9\n",
    "def create_alphabet(font_file, image_folder):\n",
    "    font = FontPreview(font_file)\n",
    "    ttf_font = TTFont(font_file)\n",
    "    font_name = font.font.getname()[0]\n",
    "    included_chars = []\n",
    "    for char in string.ascii_letters:\n",
    "        if has_glyph(ttf_font, char):\n",
    "            included_chars.append(char)\n",
    "    for char in string.digits:\n",
    "        if has_glyph(ttf_font, char):\n",
    "            included_chars.append(char)\n",
    "    split_folder = 'train'\n",
    "    if len(included_chars) != 62:\n",
    "        split_folder = 'test'\n",
    "        \n",
    "        \n",
    "    save_path = os.path.join(image_folder, split_folder, font_name)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for char in included_chars:\n",
    "        font.font_text = char\n",
    "        font.bg_color = (0, 0, 0)  # white BG\n",
    "        font.dimension = (512, 512)  # Dimension consistent with the default resolution for diffusion models\n",
    "        font.fg_color = (255, 255, 255)  # Letter color\n",
    "        font.set_font_size(300)  # font size ~ 300 pixels\n",
    "        font.set_text_position('center')  # center placement\n",
    "\n",
    "        if char in string.ascii_lowercase:\n",
    "            image_file_name = 'lower_' + char + '.jpg'\n",
    "        elif char in string.ascii_uppercase:\n",
    "            image_file_name = 'upper_' + char + '.jpg'\n",
    "        else:\n",
    "            image_file_name = char + '.jpg'\n",
    "        font.save(os.path.join(save_path, image_file_name))\n",
    "\n",
    "def create_alphabet_for_each_ttf():\n",
    "    TTF_DIR = os.path.join(os.path.abspath(os.getcwd()), 'ttf-files')\n",
    "    IMG_DIR = os.path.join(os.path.abspath(os.getcwd()), 'font-images')\n",
    "    if not os.path.exists(IMG_DIR):\n",
    "        os.mkdir(IMG_DIR)\n",
    "    fnames = os.listdir(TTF_DIR)\n",
    "\n",
    "    for fname in tqdm(fnames):\n",
    "        TTF_PATH = os.path.join(TTF_DIR, fname)\n",
    "        create_alphabet(TTF_PATH, IMG_DIR)\n",
    "\n",
    "    \n",
    "\n",
    "#Uses pandas to read through the CSV from sheets without the need of constantly redownloading\n",
    "def get_font_ttfs():\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv('font_dataset.csv')\n",
    "    # Create data folder if it does not exist\n",
    "    if not os.path.exists('ttf-files'):\n",
    "        os.makedirs('ttf-files')\n",
    "    # Loop through each row of the DataFrame\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        # Get the link and filename for the current row\n",
    "        link = row['Link']\n",
    "        filename = row['Filename']\n",
    "        if os.path.exists(os.path.join('ttf-files', filename)):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # Download the zip file from the link\n",
    "        response = requests.get(link, stream=True)\n",
    "        with open('temp.zip', 'wb') as temp_file:\n",
    "            shutil.copyfileobj(response.raw, temp_file)\n",
    "        del response\n",
    "        \n",
    "        # Unzip the downloaded file\n",
    "        with ZipFile('temp.zip', 'r') as zip_file:\n",
    "            zip_file.extract(filename)\n",
    "            \n",
    "        # Move the file to the data folder\n",
    "        source_path = os.path.join(os.getcwd(), filename)\n",
    "        dest_path = os.path.join(os.getcwd(), 'ttf-files', filename)\n",
    "        shutil.move(source_path, dest_path)\n",
    "        \n",
    "        # Remove the temporary zip file\n",
    "        os.remove('temp.zip')\n",
    "\n",
    "\n",
    "#Create the jsonl file and training folder for the images\n",
    "def create_dataset():\n",
    "    FONT_IMAGE_PATH = os.path.join(os.getcwd(), 'font-images')\n",
    "    assert os.path.exists(FONT_IMAGE_PATH)\n",
    "    TTF_PATH = os.path.join(os.getcwd(), 'ttf-files')\n",
    "    assert os.path.exists(TTF_PATH)\n",
    "    CSV_PATH = os.path.join(os.getcwd(), 'font_dataset.csv')\n",
    "\n",
    "    \n",
    "    # Step 1: Initialize the json file\n",
    "    # Step 2: Loop through the Dataframe, for each row the Filename column corresponds to the actual\n",
    "    #         folder name in 'font_images'.\n",
    "    # Step 3: For each image in the respective folder, copy it over to the training folder (renaming it) and add its entry\n",
    "    #         to the jsonl file\n",
    "\n",
    "    #Step 1\n",
    "    json_metadata = []\n",
    "    # if not os.path.exists(training_data_path):\n",
    "    #     os.makedirs(training_data_path)\n",
    "\n",
    "\n",
    "    #Step 2\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    head = df.head()\n",
    "    for idx, row_data in df.iterrows():\n",
    "        ttf_path = os.path.join(TTF_PATH, row_data['Filename'])\n",
    "        font_img_dir = FontPreview(ttf_path).font.getname()[0]\n",
    "        split_folder = 'train'\n",
    "        font_img_dir_path = os.path.join(FONT_IMAGE_PATH, split_folder, font_img_dir)\n",
    "        if not os.path.exists(font_img_dir_path):\n",
    "            split_folder = 'test'\n",
    "            font_img_dir_path = os.path.join(FONT_IMAGE_PATH, split_folder, font_img_dir)\n",
    "        font_img_paths = [os.path.join(font_img_dir_path, fname) for fname in os.listdir(font_img_dir_path)]\n",
    "        included_chars = [cur_img_path.split('/')[-1].split('.')[0] for cur_img_path in font_img_paths]\n",
    "        json_data_row = {\n",
    "            'uniqueId': str(uuid.uuid4()),\n",
    "            'font_img_paths': font_img_paths,\n",
    "            'ttf_path': ttf_path,\n",
    "            'font_characteristics': row_data['Descriptors'], \n",
    "            'chars': included_chars,\n",
    "            'font_properties': {\n",
    "                'font_weight': row_data['Weight'], \n",
    "                'rounding': row_data['Courner Rounding'], \n",
    "                'font_serifs': row_data['Serif'],\n",
    "                'width': row_data['Width'],\n",
    "                'capitals': row_data['Capitals'],\n",
    "                'dynamics': row_data['Dynamics'] \n",
    "            }\n",
    "        }\n",
    "        if split_folder == 'train':\n",
    "            train_dataset.append(json_data_row)\n",
    "        else:\n",
    "            test_dataset.append(json_data_row) \n",
    "    #Create the jsonl file\n",
    "    with open('train-metadata.jsonl', 'w') as f:\n",
    "        json.dump(train_dataset, f)\n",
    "    with open('test-metadata.jsonl', 'w') as f:\n",
    "        json.dump(test_dataset, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    #get_fonts('font_files', 'font_images')\n",
    "    #create_dataset('font_images', 'font_files', 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 166\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain-metadata.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    165\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(train_dataset, f)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-metadata.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mw\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    167\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(test_dataset, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55132c6e2b044265ae601159e8166c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_font_ttfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b387e4ca57d4bb0802fe4952897c2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shouldnt run this on TACC, best to scp the files from your own computer\n",
    "# scp localhost/font-diffusion/font-images.tar <your_user>@maverick2.tacc.utexas.edu:\\$WORK/font-diffusion\n",
    "create_alphabet_for_each_ttf()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
