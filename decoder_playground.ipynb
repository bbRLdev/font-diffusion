{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HuggingFaceImage\n",
    "from linformer import Linformer\n",
    "from vit_pytorch.efficient import ViT\n",
    "from dalle2_pytorch.tokenizer import tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "\n",
    "def get_tokenizer() -> CLIPTokenizer:\n",
    "    return CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "def get_tokenizer2():\n",
    "    return tokenizer\n",
    "def prepare_data(tokenizer):\n",
    "    def add_prompt(example):\n",
    "        props = example['font_properties']\n",
    "        character = example['character']\n",
    "        split = character.split('_')\n",
    "        if len(split) > 1:\n",
    "            character = split[0] + 'case ' + split[1]\n",
    "        else:\n",
    "            character = split[0]\n",
    "        prompt = f\"a {props['font_serifs']} {character} with {props['width']} width {props['rounding']} corners {props['font_weight']} weight and {props['dynamics']} movement with characteristics that can be described by adjectives {example['font_characteristics']}\" \n",
    "        example['prompt'] = prompt\n",
    "        return example\n",
    "    def map_tokens(example):\n",
    "        prompt = example['prompt']\n",
    "        tokens = tokenizer.encode(prompt, padding='max_length', max_length=256)\n",
    "        #tokens = tokenizer.encode(prompt)\n",
    "        example['tokens'] = tokens\n",
    "        return example\n",
    "    dataset = load_dataset('json', data_files={'train':'train-metadata.jsonl', 'test':'test-metadata.jsonl'})\n",
    "    \n",
    "    train_new_column = ['foo'] * len(dataset['train'])\n",
    "    dataset['train'] = dataset['train'].add_column('prompt', train_new_column)\n",
    "    dataset['train'] = dataset['train'].add_column('tokens', train_new_column)\n",
    "    dataset['train'] = dataset['train'].map(add_prompt)\n",
    "    dataset['train'] = dataset['train'].map(map_tokens)\n",
    "    dataset['train'] = dataset['train'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['train'] = dataset['train'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['train'] = dataset['train'].with_format('torch')\n",
    "    \n",
    "    test_new_column = ['bar'] * len(dataset['test'])\n",
    "    dataset['test'] = dataset['test'].add_column('prompt', test_new_column)\n",
    "    dataset['test'] = dataset['test'].add_column('tokens', test_new_column)\n",
    "    dataset['test'] = dataset['test'].map(add_prompt)\n",
    "    dataset['test'] = dataset['test'].map(map_tokens)\n",
    "    dataset['test'] = dataset['test'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['test'] = dataset['test'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['test'] = dataset['test'].with_format('torch')\n",
    "    return dataset\n",
    "def get_vit_model(image_size: int, patch_size: int, dim: int, depth: int, num_heads: int, k: int, device: str):\n",
    "    sequence_length = (image_size//patch_size)**2 + 1\n",
    "    # for 512x512px image with 32x32px patches: 16x16 + 1 CLS token\n",
    "    efficient_transformer = Linformer(\n",
    "        dim=dim,\n",
    "        seq_len=sequence_length,  \n",
    "        depth=depth,\n",
    "        heads=num_heads,\n",
    "        k=k\n",
    "    )\n",
    "    model = ViT(\n",
    "        dim=dim,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_classes=62,\n",
    "        transformer=efficient_transformer,\n",
    "        channels=1,\n",
    "    )\n",
    "    return model \n",
    "def get_vit(image_size, patch_size, vit_dim, vit_depth, vit_num_heads, k, device, vit_checkpoint_path):\n",
    "    vit = get_vit_model(image_size=image_size, \n",
    "                        patch_size=patch_size, \n",
    "                        dim=vit_dim, \n",
    "                        depth=vit_depth, \n",
    "                        num_heads=vit_num_heads, \n",
    "                        k=k, \n",
    "                        device=device)\n",
    "    if vit_checkpoint_path != None:\n",
    "        vit_checkpoint = torch.load(vit_checkpoint_path)\n",
    "        vit.load_state_dict(vit_checkpoint['model_state_dict'])\n",
    "        print('Loaded ViT model from checkpoint:', vit_checkpoint_path)\n",
    "    return vit\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Decoder/Prior: OpenAI CLIP "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/rinat/.cache/huggingface/datasets/json/default-1e07ea3eabca7683/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855a2e39f77e410b967e558398b55bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-54018895e1b774e9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-8115de56d8df858f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-2ad7280546491b5e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-5c61c0fc59a2958d.arrow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "dataset = prepare_data(get_tokenizer())\n",
    "#images_tensor = dataset['train'][0:2]['image']\n",
    "#texts_tensor = dataset['train'][0:2]['tokens']\n",
    "\n",
    "\n",
    "#images_tensor = images_tensor.permute(0, 3, 1, 2)\n",
    "#images_tensor = images_tensor.float()\n",
    "\n",
    "#print(images_tensor.shape)\n",
    "#print(texts_tensor.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12090 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0\n",
      "Loss: 1.0043487548828125\n",
      "Batch  0\n",
      "Loss: 1.0021426677703857\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/12090 [01:02<1:51:43,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  100\n",
      "Loss: 0.6581845879554749\n",
      "Batch  100\n",
      "Loss: 1.0761290788650513\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/12090 [02:01<1:39:16,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  200\n",
      "Loss: 1.4763939380645752\n",
      "Batch  200\n",
      "Loss: 0.6246916055679321\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 300/12090 [03:02<1:51:20,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  300\n",
      "Loss: 2.0950543880462646\n",
      "Batch  300\n",
      "Loss: 0.6427910327911377\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 400/12090 [04:02<1:43:36,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  400\n",
      "Loss: 0.44975772500038147\n",
      "Batch  400\n",
      "Loss: 0.9298602342605591\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 500/12090 [04:59<1:42:20,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  500\n",
      "Loss: 0.574957549571991\n",
      "Batch  500\n",
      "Loss: 0.17286932468414307\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 600/12090 [06:00<1:41:16,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  600\n",
      "Loss: 0.3430556654930115\n",
      "Batch  600\n",
      "Loss: 0.6291264891624451\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 700/12090 [06:58<1:43:21,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  700\n",
      "Loss: 0.07377719134092331\n",
      "Batch  700\n",
      "Loss: 0.03940613567829132\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 800/12090 [08:00<1:46:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  800\n",
      "Loss: 0.14477425813674927\n",
      "Batch  800\n",
      "Loss: 0.0780532956123352\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 900/12090 [08:54<1:28:35,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  900\n",
      "Loss: nan\n",
      "Batch  900\n",
      "Loss: nan\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1000/12090 [09:52<1:49:24,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1000\n",
      "Loss: nan\n",
      "Batch  1000\n",
      "Loss: nan\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1100/12090 [10:50<1:29:03,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1100\n",
      "Loss: nan\n",
      "Batch  1100\n",
      "Loss: nan\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1200/12090 [11:43<1:27:22,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1200\n",
      "Loss: nan\n",
      "Batch  1200\n",
      "Loss: nan\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1300/12090 [12:41<1:48:11,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1300\n",
      "Loss: nan\n",
      "Batch  1300\n",
      "Loss: nan\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1400/12090 [13:39<1:40:19,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1400\n",
      "Loss: nan\n",
      "Batch  1400\n",
      "Loss: nan\n",
      "Saving decoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1469/12090 [14:22<1:43:59,  1.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m unet_number \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     loss \u001b[39m=\u001b[39m decoder_trainer(\n\u001b[0;32m     82\u001b[0m         img,\n\u001b[0;32m     83\u001b[0m         text \u001b[39m=\u001b[39;49m emb,\n\u001b[0;32m     84\u001b[0m         unet_number \u001b[39m=\u001b[39;49m unet_number, \u001b[39m# which unet to train on\u001b[39;49;00m\n\u001b[0;32m     85\u001b[0m         max_batch_size \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m         \u001b[39m# gradient accumulation - this sets the maximum batch size in which to do forward and backwards pass - for this example 32 / 4 == 8 times\u001b[39;49;00m\n\u001b[0;32m     86\u001b[0m     )\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m print_loss_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     89\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m'\u001b[39m, i)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\dalle2_pytorch\\trainer.py:107\u001b[0m, in \u001b[0;36mcast_torch_tensor.<locals>.inner\u001b[1;34m(model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m args, kwargs_values \u001b[39m=\u001b[39m all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n\u001b[0;32m    105\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(kwargs_keys, kwargs_values)))\n\u001b[1;32m--> 107\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\dalle2_pytorch\\trainer.py:723\u001b[0m, in \u001b[0;36mDecoderTrainer.forward\u001b[1;34m(self, unet_number, max_batch_size, return_lowres_cond_image, *args, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[39mfor\u001b[39;00m chunk_size_frac, (chunked_args, chunked_kwargs) \u001b[39min\u001b[39;00m split_args_and_kwargs(\u001b[39m*\u001b[39margs, split_size \u001b[39m=\u001b[39m max_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    722\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mautocast():\n\u001b[1;32m--> 723\u001b[0m         loss_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39mchunked_args, unet_number \u001b[39m=\u001b[39m unet_number, return_lowres_cond_image\u001b[39m=\u001b[39mreturn_lowres_cond_image, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mchunked_kwargs)\n\u001b[0;32m    724\u001b[0m         \u001b[39m# loss_obj may be a tuple with loss and cond_image\u001b[39;00m\n\u001b[0;32m    725\u001b[0m         \u001b[39mif\u001b[39;00m return_lowres_cond_image:\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\dalle2_pytorch\\dalle2_pytorch.py:3245\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, image, text, image_embed, text_encodings, unet_number, return_lowres_cond_image)\u001b[0m\n\u001b[0;32m   3243\u001b[0m \u001b[39mif\u001b[39;00m exists(text) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(text_encodings) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconditional:\n\u001b[0;32m   3244\u001b[0m     \u001b[39massert\u001b[39;00m exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip), \u001b[39m'\u001b[39m\u001b[39mif you are passing in raw text, you need to supply `clip` to the decoder\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 3245\u001b[0m     _, text_encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip\u001b[39m.\u001b[39;49membed_text(text)\n\u001b[0;32m   3247\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcondition_on_text_encodings \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(text_encodings)), \u001b[39m'\u001b[39m\u001b[39mtext or text encodings must be passed into decoder if specified\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   3248\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcondition_on_text_encodings \u001b[39mand\u001b[39;00m exists(text_encodings)), \u001b[39m'\u001b[39m\u001b[39mdecoder specified not to be conditioned on text, yet it is presented\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\dalle2_pytorch\\dalle2_pytorch.py:366\u001b[0m, in \u001b[0;36mOpenAIClipAdapter.embed_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    363\u001b[0m text_mask \u001b[39m=\u001b[39m text_mask \u001b[39m&\u001b[39m (text \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcleared\n\u001b[1;32m--> 366\u001b[0m text_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip\u001b[39m.\u001b[39;49mencode_text(text)\n\u001b[0;32m    367\u001b[0m text_encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encodings\n\u001b[0;32m    368\u001b[0m text_encodings \u001b[39m=\u001b[39m text_encodings\u001b[39m.\u001b[39mmasked_fill(\u001b[39m~\u001b[39mtext_mask[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m], \u001b[39m0.\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\clip\\model.py:350\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    348\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding[:n_ctx]\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    349\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[0;32m    351\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[0;32m    352\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\clip\\model.py:204\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\clip\\model.py:192\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    191\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[1;32m--> 192\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[0;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dalle2_pytorch import DALLE2, Unet, Decoder, OpenAIClipAdapter\n",
    "from dalle2_pytorch import DecoderTrainer\n",
    "# openai pretrained clip - defaults to ViT-B/32\n",
    "n_epochs = 1\n",
    "print_loss_every = 500\n",
    "clip = OpenAIClipAdapter()\n",
    "\n",
    "# decoder (with unet)\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 128,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults=(1, 2, 4, 8),\n",
    "    text_embed_dim = 512,\n",
    "    cond_on_text_encodings = True  # set to True for any unets that need to be conditioned on text encodings (ex. first unet in cascade)\n",
    ").cuda()\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 16,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults = (1, 2, 4, 8, 16)\n",
    ").cuda()\n",
    "\n",
    "decoder = Decoder(\n",
    "    unet = (unet1, unet2),\n",
    "    image_sizes = (128, 256),\n",
    "    clip = clip,\n",
    "    timesteps = 1000,\n",
    "    sample_timesteps = (250, 27),\n",
    "    image_cond_drop_prob = 0.1,\n",
    "    text_cond_drop_prob = 0.5\n",
    ").cuda()\n",
    "\n",
    "decoder_trainer = DecoderTrainer(\n",
    "    decoder,\n",
    "    lr = 1e-5,\n",
    "    wd = 1e-2,\n",
    "    ema_beta = 0.99,\n",
    "    ema_update_after_step = 1000,\n",
    "    ema_update_every = 10,\n",
    ")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch: ', epoch)\n",
    "    u1_loss = 0 \n",
    "    u2_loss = 0\n",
    "\n",
    "    for i, data in enumerate(tqdm(dataset['train'])):\n",
    "        img, emb = data['image'], data['tokens'] \n",
    "        img = img.unsqueeze(0)\n",
    "        emb = emb.unsqueeze(0)\n",
    "        img = img.permute(0, 3, 1, 2)\n",
    "        img = img.float()\n",
    "\n",
    "        for unet_number in (1, 2):\n",
    "            loss = decoder_trainer(\n",
    "                img,\n",
    "                text = emb,\n",
    "                unet_number = unet_number, # which unet to train on\n",
    "                max_batch_size = 4         # gradient accumulation - this sets the maximum batch size in which to do forward and backwards pass - for this example 32 / 4 == 8 times\n",
    "            )\n",
    "\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Batch ', i)\n",
    "                print('Loss:', loss)\n",
    "            decoder_trainer.update(unet_number)\n",
    "\n",
    "        if i%100 == 0:\n",
    "            print('Saving decoder model...')\n",
    "            decoder_trainer.save(\"best_decoder.pth\")\n",
    "print('Saving last decoder')\n",
    "decoder_trainer.save(\"last_decoder.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12090 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0\n",
      "Loss: 1.3394761085510254\n",
      "Saving prior model...\n",
      "Saving checkpoint at step: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 99/12090 [00:10<17:44, 11.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prior model...\n",
      "Saving checkpoint at step: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/12090 [00:20<17:52, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prior model...\n",
      "Saving checkpoint at step: 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 300/12090 [00:30<17:37, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prior model...\n",
      "Saving checkpoint at step: 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 400/12090 [00:41<18:08, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prior model...\n",
      "Saving checkpoint at step: 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 499/12090 [00:52<18:24, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  500\n",
      "Loss: 0.0503176786005497\n",
      "Saving prior model...\n",
      "Saving checkpoint at step: 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 600/12090 [01:03<17:56, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prior model...\n",
      "Saving checkpoint at step: 601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 700/12090 [01:13<18:04, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prior model...\n",
      "Saving checkpoint at step: 701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 799/12090 [01:24<18:02, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prior model...\n",
      "Saving checkpoint at step: 801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 815/12090 [01:27<20:13,  9.29it/s]"
     ]
    }
   ],
   "source": [
    "#### Train the Prior \n",
    "from dalle2_pytorch import DiffusionPrior, DiffusionPriorNetwork, DiffusionPriorTrainer\n",
    "from dalle2_pytorch import OpenAIClipAdapter\n",
    "\n",
    "n_epochs = 1\n",
    "print_loss_every = 500\n",
    "\n",
    "clip = OpenAIClipAdapter()\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").cuda()\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    clip = clip,\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.2\n",
    ").cuda()\n",
    "\n",
    "diffusion_prior_trainer = DiffusionPriorTrainer(\n",
    "    diffusion_prior,\n",
    "    lr = 1e-5,\n",
    "    wd = 1e-2,\n",
    "    ema_beta = 0.99,\n",
    "    ema_update_after_step = 1000,\n",
    "    ema_update_every = 10,\n",
    ")\n",
    "\n",
    "for i, data in enumerate(tqdm(dataset['train'])):\n",
    "        img, emb = data['image'], data['tokens'] \n",
    "        img = img.unsqueeze(0)\n",
    "        emb = emb.unsqueeze(0)\n",
    "        img = img.permute(0, 3, 1, 2)\n",
    "        img = img.float()\n",
    "\n",
    "        loss = diffusion_prior_trainer(emb, img, max_batch_size = 4)\n",
    "\n",
    "\n",
    "        if i % print_loss_every == 0:\n",
    "            print('Batch ', i)\n",
    "            print('Loss:', loss)\n",
    "        diffusion_prior_trainer.update()\n",
    "\n",
    "        if i%100 == 0:\n",
    "            print('Saving prior model...')\n",
    "            diffusion_prior_trainer.save(\"best_prior.pth\")\n",
    "print('Saving last prior model')\n",
    "diffusion_prior_trainer.save(\"last_prior.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the decoder \n",
    "decoder_path = ''\n",
    "unet1 = Unet(\n",
    "    dim = 128,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults=(1, 2, 4, 8),\n",
    "    text_embed_dim = 512,\n",
    "    cond_on_text_encodings = True  # set to True for any unets that need to be conditioned on text encodings (ex. first unet in cascade)\n",
    ").cuda()\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 16,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults = (1, 2, 4, 8, 16)\n",
    ").cuda()\n",
    "\n",
    "decoder = Decoder(\n",
    "    unet = (unet1, unet2),\n",
    "    image_sizes = (128, 256),\n",
    "    clip = clip,\n",
    "    timesteps = 1000,\n",
    "    sample_timesteps = (250, 27),\n",
    "    image_cond_drop_prob = 0.1,\n",
    "    text_cond_drop_prob = 0.5\n",
    ").cuda()\n",
    "\n",
    "decoder.load_state_dict(torch.load(decoder_path, map_location='cpu'))\n",
    "\n",
    "#Load the prior \n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").cuda()\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    clip = clip,\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.2\n",
    ").cuda()\n",
    "prior_path = 'PATH_TO_PRIOR'\n",
    "diffusion_prior.load_state_dict(torch.load(prior_path, map_location='cpu'))\n",
    "\n",
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ")\n",
    "\n",
    "images = dalle2(\n",
    "    ['A lowercase a which has traits blocky and properties black square sans serif static extended all caps'],\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 4 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m#print(images.squeeze(0).shape)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m img \u001b[39m=\u001b[39m ToPILImage()(images)\n\u001b[0;32m      8\u001b[0m img\u001b[39m.\u001b[39mshow()\n\u001b[0;32m      9\u001b[0m \u001b[39m#save_image(images, 'example_output.png')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    226\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_pil_image(pic, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torchvision\\transforms\\functional.py:266\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pic, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m}:\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndimension()\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m     \u001b[39melif\u001b[39;00m pic\u001b[39m.\u001b[39mndimension() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    269\u001b[0m         \u001b[39m# if 2D image, add channel dimension (CHW)\u001b[39;00m\n\u001b[0;32m    270\u001b[0m         pic \u001b[39m=\u001b[39m pic\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 4 dimensions."
     ]
    }
   ],
   "source": [
    "#Save image \n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "#print(images.squeeze(0).shape)\n",
    "images = images.squeeze(0)\n",
    "img = ToPILImage()(images)\n",
    "img.show()\n",
    "#save_image(images, 'example_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HugginFaceImage - 512 x 512 x 3 \n",
    "#Token legnth - 42\n",
    "\n",
    "#Required format: \n",
    "#Image: (B, C, W, H) ~ channels: 3 , W,H: 512 \n",
    "#Text: (B, T)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "font_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
