{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HuggingFaceImage\n",
    "from linformer import Linformer\n",
    "from vit_pytorch.efficient import ViT\n",
    "from dalle2_pytorch.tokenizer import tokenizer\n",
    "import torch\n",
    "def get_tokenizer() -> CLIPTokenizer:\n",
    "    return CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "def get_tokenizer2():\n",
    "    return tokenizer\n",
    "def prepare_data(tokenizer):\n",
    "    def add_prompt(example):\n",
    "        props = example['font_properties']\n",
    "        character = example['character']\n",
    "        split = character.split('_')\n",
    "        if len(split) > 1:\n",
    "            character = split[0] + 'case ' + split[1]\n",
    "        else:\n",
    "            character = split[0]\n",
    "        prompt = f\"a {props['font_serifs']} {character} with {props['width']} width {props['rounding']} corners {props['font_weight']} weight and {props['dynamics']} movement with characteristics that can be described by adjectives {example['font_characteristics']}\" \n",
    "        example['prompt'] = prompt\n",
    "        return example\n",
    "    def map_tokens(example):\n",
    "        prompt = example['prompt']\n",
    "        tokens = tokenizer.encode(prompt, padding='max_length', max_length=256)\n",
    "        #tokens = tokenizer.encode(prompt)\n",
    "        example['tokens'] = tokens\n",
    "        return example\n",
    "    dataset = load_dataset('json', data_files={'train':'train-metadata.jsonl', 'test':'test-metadata.jsonl'})\n",
    "    \n",
    "    train_new_column = ['foo'] * len(dataset['train'])\n",
    "    dataset['train'] = dataset['train'].add_column('prompt', train_new_column)\n",
    "    dataset['train'] = dataset['train'].add_column('tokens', train_new_column)\n",
    "    dataset['train'] = dataset['train'].map(add_prompt)\n",
    "    dataset['train'] = dataset['train'].map(map_tokens)\n",
    "    dataset['train'] = dataset['train'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['train'] = dataset['train'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['train'] = dataset['train'].with_format('torch')\n",
    "    \n",
    "    test_new_column = ['bar'] * len(dataset['test'])\n",
    "    dataset['test'] = dataset['test'].add_column('prompt', test_new_column)\n",
    "    dataset['test'] = dataset['test'].add_column('tokens', test_new_column)\n",
    "    dataset['test'] = dataset['test'].map(add_prompt)\n",
    "    dataset['test'] = dataset['test'].map(map_tokens)\n",
    "    dataset['test'] = dataset['test'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['test'] = dataset['test'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['test'] = dataset['test'].with_format('torch')\n",
    "    return dataset\n",
    "def get_vit_model(image_size: int, patch_size: int, dim: int, depth: int, num_heads: int, k: int, device: str):\n",
    "    sequence_length = (image_size//patch_size)**2 + 1\n",
    "    # for 512x512px image with 32x32px patches: 16x16 + 1 CLS token\n",
    "    efficient_transformer = Linformer(\n",
    "        dim=dim,\n",
    "        seq_len=sequence_length,  \n",
    "        depth=depth,\n",
    "        heads=num_heads,\n",
    "        k=k\n",
    "    )\n",
    "    model = ViT(\n",
    "        dim=dim,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_classes=62,\n",
    "        transformer=efficient_transformer,\n",
    "        channels=1,\n",
    "    )\n",
    "    return model \n",
    "def get_vit(image_size, patch_size, vit_dim, vit_depth, vit_num_heads, k, device, vit_checkpoint_path):\n",
    "    vit = get_vit_model(image_size=image_size, \n",
    "                        patch_size=patch_size, \n",
    "                        dim=vit_dim, \n",
    "                        depth=vit_depth, \n",
    "                        num_heads=vit_num_heads, \n",
    "                        k=k, \n",
    "                        device=device)\n",
    "    if vit_checkpoint_path != None:\n",
    "        vit_checkpoint = torch.load(vit_checkpoint_path)\n",
    "        vit.load_state_dict(vit_checkpoint['model_state_dict'])\n",
    "        print('Loaded ViT model from checkpoint:', vit_checkpoint_path)\n",
    "    return vit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from transformers import CLIPTextModel\n",
    "# from x_clip_train import get_tokenizer\n",
    "# import torch\n",
    "# from x_clip import CLIP\n",
    "# from vit_pytorch.extractor import Extractor\n",
    "\n",
    "# image_size = 512\n",
    "# patch_size = 32\n",
    "# vit_dim = 128\n",
    "# vit_depth = 12\n",
    "# vit_num_heads = 8\n",
    "# k = 64\n",
    "# vit_checkpoint = './vit-checkpoints/model-512-epoch3.pt'\n",
    "# base_vit = get_vit(image_size, \n",
    "#                     patch_size, \n",
    "#                     vit_dim, \n",
    "#                     vit_depth, \n",
    "#                     vit_num_heads, \n",
    "#                     k, \n",
    "#                     device=None, \n",
    "#                     vit_checkpoint_path=None)\n",
    "# image_encoder = Extractor(\n",
    "#     base_vit,\n",
    "#     return_embeddings_only = True\n",
    "# )\n",
    "# clip_tokenizer = get_tokenizer(True)\n",
    "# text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "# text_encoder.resize_token_embeddings(len(clip_tokenizer))\n",
    "# path = os.path.join(os.getcwd(), 'clip-checkpoints', 'clip-epoch-9.pt')\n",
    "# checkpoint = torch.load(path)\n",
    "# clip = CLIP(\n",
    "#     image_encoder = image_encoder,\n",
    "#     text_encoder = text_encoder,\n",
    "#     dim_image=128,\n",
    "#     dim_text=512,\n",
    "#     dim_latent=384,\n",
    "#     text_encode_without_mask=False,\n",
    "#     use_all_token_embeds=False,\n",
    "#     text_has_cls_token=True,\n",
    "#     visual_has_cls_token=True,\n",
    "#     num_text_tokens=text_encoder.vocab_size,\n",
    "#     text_pad_id=clip_tokenizer.pad_token_id,\n",
    "#     text_eos_id=clip_tokenizer.eos_token_id,\n",
    "#     use_mlm=True,\n",
    "#     mlm_mask_token_id=clip_tokenizer.mask_token_id,\n",
    "#     mlm_pad_token_id=clip_tokenizer.pad_token_id,\n",
    "#     mlm_mask_ignore_token_ids=[clip_tokenizer.bos_token_id],\n",
    "#     channels=1\n",
    "# ).to('cuda')\n",
    "# clip.load_state_dict(checkpoint['model_state_dict'])\n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# start_epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "# print('Loaded model from checkpoint:', path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving forward: Train Decoder (Custom Linformer + CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65, 128])\n",
      "KV LEN 65 Seq len 257\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "the sequence length of the key / values must be 257 - 65 given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     29\u001b[0m \u001b[39m# feed images into decoder\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m loss \u001b[39m=\u001b[39m decoder(images)\n\u001b[0;32m     32\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\dalle2_pytorch\\dalle2_pytorch.py:3241\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, image, text, image_embed, text_encodings, unet_number, return_lowres_cond_image)\u001b[0m\n\u001b[0;32m   3239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(image_embed) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconditional:\n\u001b[0;32m   3240\u001b[0m     \u001b[39massert\u001b[39;00m exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip), \u001b[39m'\u001b[39m\u001b[39mif you want to derive CLIP image embeddings automatically, you must supply `clip` to the decoder on init\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 3241\u001b[0m     image_embed, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip\u001b[39m.\u001b[39;49membed_image(image)\n\u001b[0;32m   3243\u001b[0m \u001b[39mif\u001b[39;00m exists(text) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(text_encodings) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconditional:\n\u001b[0;32m   3244\u001b[0m     \u001b[39massert\u001b[39;00m exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip), \u001b[39m'\u001b[39m\u001b[39mif you are passing in raw text, you need to supply `clip` to the decoder\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\dalle2_pytorch\\dalle2_pytorch.py:270\u001b[0m, in \u001b[0;36mXClipAdapter.embed_image\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_image\u001b[39m(\u001b[39mself\u001b[39m, image):\n\u001b[0;32m    269\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_and_resize_image(image)\n\u001b[1;32m--> 270\u001b[0m     encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip\u001b[39m.\u001b[39;49mvisual_transformer(image)\n\u001b[0;32m    271\u001b[0m     image_cls, image_encodings \u001b[39m=\u001b[39m encoder_output[:, \u001b[39m0\u001b[39m], encoder_output[:, \u001b[39m1\u001b[39m:]\n\u001b[0;32m    272\u001b[0m     image_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip\u001b[39m.\u001b[39mto_visual_latent(image_cls)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\vit_pytorch\\extractor.py:82\u001b[0m, in \u001b[0;36mExtractor.forward\u001b[1;34m(self, img, return_embeddings_only)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_registered:\n\u001b[0;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_hook()\n\u001b[1;32m---> 82\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvit(img)\n\u001b[0;32m     84\u001b[0m target_device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39melse\u001b[39;00m img\u001b[39m.\u001b[39mdevice\n\u001b[0;32m     85\u001b[0m latents \u001b[39m=\u001b[39m apply_tuple_or_single(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mto(target_device), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatents)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\vit_pytorch\\efficient.py:45\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     43\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((cls_tokens, x), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding[:, :(n \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[1;32m---> 45\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[0;32m     47\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean(dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m x[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_latent(x)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\linformer\\linformer.py:153\u001b[0m, in \u001b[0;36mLinformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\linformer\\reversible.py:149\u001b[0m, in \u001b[0;36mSequentialSequence.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     layers_and_args \u001b[39m=\u001b[39m layer_drop(layers_and_args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_dropout)\n\u001b[0;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m (f, g), (f_args, g_args) \u001b[39min\u001b[39;00m layers_and_args:\n\u001b[1;32m--> 149\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m f(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mf_args)\n\u001b[0;32m    150\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m g(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mg_args)\n\u001b[0;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\linformer\\linformer.py:35\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(x)\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rinat\\anaconda3\\envs\\font_diffusion\\lib\\site-packages\\linformer\\linformer.py:101\u001b[0m, in \u001b[0;36mLinformerSelfAttention.forward\u001b[1;34m(self, x, context, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     99\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mKV LEN\u001b[39m\u001b[39m'\u001b[39m, kv_len, \u001b[39m'\u001b[39m\u001b[39mSeq len\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len)\n\u001b[1;32m--> 101\u001b[0m \u001b[39massert\u001b[39;00m kv_len \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe sequence length of the key / values must be \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len\u001b[39m}\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m{\u001b[39;00mkv_len\u001b[39m}\u001b[39;00m\u001b[39m given\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    103\u001b[0m queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_q(x)\n\u001b[0;32m    105\u001b[0m proj_seq_len \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m args: torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mbnd,nk->bkd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39margs)\n",
      "\u001b[1;31mAssertionError\u001b[0m: the sequence length of the key / values must be 257 - 65 given"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from dalle2_pytorch import Unet, Decoder, CLIP\n",
    "\n",
    "\n",
    "# # unet for the decoder\n",
    "# unet = Unet(\n",
    "#     dim = 128,\n",
    "#     image_embed_dim = 512,\n",
    "#     cond_dim = 128,\n",
    "#     channels = 1,\n",
    "#     dim_mults=(1, 2, 4, 8)\n",
    "# ).cuda()\n",
    "\n",
    "# # decoder, which contains the unet and clip\n",
    "\n",
    "# decoder = Decoder(\n",
    "#     unet = unet,\n",
    "#     clip = clip,\n",
    "#     timesteps = 100,\n",
    "#     image_cond_drop_prob = 0.1,\n",
    "#     text_cond_drop_prob = 0.5,\n",
    "#     channels=1\n",
    "# ).cuda()\n",
    "\n",
    "# # mock images (get a lot of this)\n",
    "\n",
    "# images = torch.randn(1, 1, 512, 512).cuda()\n",
    "\n",
    "# # feed images into decoder\n",
    "\n",
    "# loss = decoder(images)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Decoder/Prior: OpenAI CLIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/rinat/.cache/huggingface/datasets/json/default-1e07ea3eabca7683/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced8f56003a44a7fbe14a9b45f0d343d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-54018895e1b774e9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-8115de56d8df858f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-2ad7280546491b5e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\rinat\\.cache\\huggingface\\datasets\\json\\default-1e07ea3eabca7683\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-5c61c0fc59a2958d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "dataset = prepare_data(get_tokenizer())\n",
    "images_tensor = dataset['train'][0:1]['image']\n",
    "texts_tensor = dataset['train'][0:1]['tokens']\n",
    "\n",
    "\n",
    "images_tensor = images_tensor.permute(0, 3, 1, 2)\n",
    "images_tensor = images_tensor.float()\n",
    "\n",
    "print(images_tensor.shape)\n",
    "print(texts_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee7c614c0be4ab2bb32503ffe44be04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b71aa0e27e84d448583dab45f28a7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd875bb5a1042399c69e92ba83739ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591750d5ba4143c281e61252226f095f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder, OpenAIClipAdapter\n",
    "\n",
    "# openai pretrained clip - defaults to ViT-B/32\n",
    "\n",
    "clip = OpenAIClipAdapter()\n",
    "\n",
    "# mock data\n",
    "\n",
    "text = texts_tensor.cuda()\n",
    "images_tensor = images_tensor.float()\n",
    "images = images_tensor.cuda()\n",
    "\n",
    "# prior networks (with transformer)\n",
    "\n",
    "prior_network = DiffusionPriorNetwork(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").cuda()\n",
    "\n",
    "diffusion_prior = DiffusionPrior(\n",
    "    net = prior_network,\n",
    "    clip = clip,\n",
    "    timesteps = 10000,\n",
    "    cond_drop_prob = 0.2\n",
    ").cuda()\n",
    "\n",
    "loss = diffusion_prior(text, images)\n",
    "loss.backward()\n",
    "\n",
    "# decoder (with unet)\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 128,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults=(1, 2, 4, 8),\n",
    "    text_embed_dim = 512,\n",
    "    cond_on_text_encodings = True  # set to True for any unets that need to be conditioned on text encodings (ex. first unet in cascade)\n",
    ").cuda()\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 16,\n",
    "    image_embed_dim = 512,\n",
    "    cond_dim = 128,\n",
    "    channels = 3,\n",
    "    dim_mults = (1, 2, 4, 8, 16)\n",
    ").cuda()\n",
    "\n",
    "decoder = Decoder(\n",
    "    unet = (unet1, unet2),\n",
    "    image_sizes = (128, 256),\n",
    "    clip = clip,\n",
    "    timesteps = 10000,\n",
    "    sample_timesteps = (250, 27),\n",
    "    image_cond_drop_prob = 0.1,\n",
    "    text_cond_drop_prob = 0.5\n",
    ").cuda()\n",
    "\n",
    "for unet_number in (1, 2):\n",
    "    loss = decoder(images, text = text, unet_number = unet_number) # this can optionally be decoder(images, text) if you wish to condition on the text encodings as well, though it was hinted in the paper it didn't do much\n",
    "    loss.backward()\n",
    "\n",
    "# do above for many steps\n",
    "\n",
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ")\n",
    "\n",
    "images = dalle2(\n",
    "    ['A lowercase a which has traits blocky and properties black square sans serif static extended all caps'],\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save image \n",
    "from torchvision.utils import save_image\n",
    "save_image(images, 'example_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HugginFaceImage - 512 x 512 x 3 \n",
    "#Token legnth - 42\n",
    "\n",
    "#Required format: \n",
    "#Image: (B, C, W, H) ~ channels: 3 , W,H: 512 \n",
    "#Text: (B, T)\n",
    "\n",
    "images_tensor = dataset['train'][0]['image']\n",
    "texts_tensor = dataset['train'][0]['tokens']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "font_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
