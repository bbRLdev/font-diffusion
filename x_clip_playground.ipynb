{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HuggingFaceImage\n",
    "from linformer import Linformer\n",
    "from vit_pytorch.efficient import ViT\n",
    "import torch\n",
    "def get_tokenizer() -> CLIPTokenizer:\n",
    "    return CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "def prepare_data(tokenizer: CLIPTokenizer):\n",
    "    def add_prompt(example):\n",
    "        props = example['font_properties']\n",
    "        character = example['character']\n",
    "        split = character.split('_')\n",
    "        if len(split) > 1:\n",
    "            character = split[0] + 'case ' + split[1]\n",
    "        else:\n",
    "            character = split[0]\n",
    "        prompt = f\"a {props['font_serifs']} {character} with {props['width']} width {props['rounding']} corners {props['font_weight']} weight and {props['dynamics']} movement with characteristics that can be described by adjectives {example['font_characteristics']}\" \n",
    "        example['prompt'] = prompt\n",
    "        return example\n",
    "    def map_tokens(example):\n",
    "        prompt = example['prompt']\n",
    "        tokens = tokenizer.encode(prompt, padding='max_length', max_length=42)\n",
    "        example['tokens'] = tokens\n",
    "        return example\n",
    "    dataset = load_dataset('json', data_files={'train':'train-metadata.jsonl', 'test':'test-metadata.jsonl'})\n",
    "    \n",
    "    train_new_column = ['foo'] * len(dataset['train'])\n",
    "    dataset['train'] = dataset['train'].add_column('prompt', train_new_column)\n",
    "    dataset['train'] = dataset['train'].add_column('tokens', train_new_column)\n",
    "    dataset['train'] = dataset['train'].map(add_prompt)\n",
    "    dataset['train'] = dataset['train'].map(map_tokens)\n",
    "    dataset['train'] = dataset['train'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['train'] = dataset['train'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['train'] = dataset['train'].with_format('torch')\n",
    "    \n",
    "    test_new_column = ['bar'] * len(dataset['test'])\n",
    "    dataset['test'] = dataset['test'].add_column('prompt', test_new_column)\n",
    "    dataset['test'] = dataset['test'].add_column('tokens', test_new_column)\n",
    "    dataset['test'] = dataset['test'].map(add_prompt)\n",
    "    dataset['test'] = dataset['test'].map(map_tokens)\n",
    "    dataset['test'] = dataset['test'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['test'] = dataset['test'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['test'] = dataset['test'].with_format('torch')\n",
    "    return dataset\n",
    "def get_vit_model(image_size: int, patch_size: int, dim: int, depth: int, num_heads: int, k: int, device: str):\n",
    "    sequence_length = (image_size//patch_size)**2 + 1\n",
    "    # for 512x512px image with 32x32px patches: 16x16 + 1 CLS token\n",
    "    efficient_transformer = Linformer(\n",
    "        dim=dim,\n",
    "        seq_len=sequence_length,  \n",
    "        depth=depth,\n",
    "        heads=num_heads,\n",
    "        k=k\n",
    "    )\n",
    "    model = ViT(\n",
    "        dim=dim,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_classes=62,\n",
    "        transformer=efficient_transformer,\n",
    "        channels=1,\n",
    "    )\n",
    "    return model \n",
    "def get_vit(image_size, patch_size, vit_dim, vit_depth, vit_num_heads, k, device, vit_checkpoint_path):\n",
    "    vit = get_vit_model(image_size=image_size, \n",
    "                        patch_size=patch_size, \n",
    "                        dim=vit_dim, \n",
    "                        depth=vit_depth, \n",
    "                        num_heads=vit_num_heads, \n",
    "                        k=k, \n",
    "                        device=device)\n",
    "    vit_checkpoint = torch.load(vit_checkpoint_path)\n",
    "    if vit_checkpoint != None:\n",
    "        vit.load_state_dict(vit_checkpoint['model_state_dict'])\n",
    "        print('Loaded ViT model from checkpoint:', vit_checkpoint_path)\n",
    "    return vit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Michael Labarca/.cache/huggingface/datasets/json/default-c772ad4eb6d31de9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac595c6e48de402ba749361819c40819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-6c27256c2b0087be.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-53a3018202bd379b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-a1fe57a82fffa92f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-8f3e544481ecc8a1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 12090\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 208\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = dataset['train'].filter(lambda ex: len(ex['tokens']) != 28)\n",
    "unique = set()\n",
    "for ex in filtered:\n",
    "    if len(ex['tokens']) == 35:\n",
    "        print(ex['prompt'])\n",
    "    unique.add(len(ex['tokens']))\n",
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinformerLM(nn.Module):\n",
    "    def __init__(self, num_tokens, dim, seq_len, depth, k = 256, heads = 8, dim_head = None, one_kv_head = False, share_kv = False, reversible = False, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.linformer = Linformer(dim, seq_len, depth, k = k, heads = heads, dim_head = dim_head,\n",
    "                one_kv_head = one_kv_head, share_kv = share_kv, reversible = reversible, dropout = dropout)\n",
    "        # self.to_logits = nn.Linear(dim, num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_emb(torch.arange(x.shape[1], device=x.device)) + x\n",
    "        x = self.linformer(x)\n",
    "        # out = self.to_logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Michael Labarca/.cache/huggingface/datasets/json/default-c772ad4eb6d31de9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e015e2759d2146059f5d6631eac7ce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-6c27256c2b0087be.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-53a3018202bd379b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-a1fe57a82fffa92f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-8f3e544481ecc8a1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 12090\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 208\n",
      "    })\n",
      "})\n",
      "Loaded ViT model from checkpoint: ./vit-checkpoints/model-epoch18.pt\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m     batch_imgs \u001b[39m=\u001b[39m batch_imgs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     71\u001b[0m     loss \u001b[39m=\u001b[39m clip(batch_tokens, batch_imgs, return_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 72\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     73\u001b[0m lr\u001b[39m=\u001b[39m\u001b[39m3e-5\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trainable_params\u001b[39m(model):\n",
      "File \u001b[1;32mc:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from x_clip import CLIP\n",
    "from vit_pytorch.extractor import Extractor\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "device = 'cuda'\n",
    "clip_tokenizer = get_tokenizer()\n",
    "dataset = prepare_data(clip_tokenizer)\n",
    "print(dataset)\n",
    "def get_dataloaders(train_clip_dataset, test_clip_dataset, batch_size):\n",
    "    train_loader = DataLoader(dataset=train_clip_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_clip_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "train_loader, valid_loader = get_dataloaders(train_dataset, test_dataset, 2)\n",
    "vit_checkpoint = './vit-checkpoints/model-epoch18.pt'\n",
    "image_size = 512\n",
    "patch_size = 32\n",
    "vit_dim = 128\n",
    "vit_depth = 12\n",
    "vit_num_heads = 8\n",
    "k = 64\n",
    "base_vit = get_vit(image_size, \n",
    "                    patch_size, \n",
    "                    vit_dim, \n",
    "                    vit_depth, \n",
    "                    vit_num_heads, \n",
    "                    k, \n",
    "                    device, \n",
    "                    vit_checkpoint)\n",
    "image_encoder = Extractor(\n",
    "    base_vit,\n",
    "    return_embeddings_only = True\n",
    ")\n",
    "text_encoder = LinformerLM(\n",
    "    num_tokens=49408,\n",
    "    dim = 256,\n",
    "    seq_len = 42,\n",
    "    depth = 12,\n",
    "    heads = 8,\n",
    "    dim_head = 64,        # be able to set the dimension of each head in multi-head attention\n",
    "    k = 128,               # this is the k that the key/values are projected to along the sequence dimension\n",
    "    one_kv_head = True,    # share one key/value head across all heads\n",
    "    share_kv = False,      # share the same projection for keys and values\n",
    "    reversible = False,      # make network reversible, like Reformer\n",
    ")\n",
    "clip = CLIP(\n",
    "    image_encoder = image_encoder,\n",
    "    text_encoder = text_encoder,\n",
    "    dim_image=128,\n",
    "    dim_text=256,\n",
    "    dim_latent=128,\n",
    "    text_encode_without_mask=True,\n",
    "    use_all_token_embeds=True,\n",
    "    text_has_cls_token=False,\n",
    "    visual_has_cls_token=True,\n",
    ").to(device)\n",
    "def prepare_batch(batch):\n",
    "    batch_imgs = batch['image']\n",
    "    batch_tokens = batch['tokens']\n",
    "    batch_imgs = batch_imgs[:, :, :, 0].unsqueeze(-1)\n",
    "    batch_imgs = batch_imgs.permute(0, 3, 1, 2)\n",
    "    batch_imgs = batch_imgs.type('torch.FloatTensor')\n",
    "    return batch_imgs, batch_tokens\n",
    "for batch in train_loader:\n",
    "    batch_imgs, batch_tokens = prepare_batch(batch)\n",
    "    # batch_imgs.to(device)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    batch_imgs = batch_imgs.to(device)\n",
    "    loss = clip(batch_tokens, batch_imgs, return_loss=True)\n",
    "    loss.backward()\n",
    "lr=3e-5\n",
    "def get_trainable_params(model):\n",
    "    return [params for params in model.parameters() if params.requires_grad]\n",
    "optimizer = AdamW(get_trainable_params(clip), lr=lr) # DALLE-pytorch setup\n",
    "for epoch in range(0, 10):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_imgs, batch_tokens = prepare_batch(batch)\n",
    "        # batch_imgs.to(device)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        loss = clip(batch_tokens, batch_imgs, return_loss=True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (output.argmax(dim=1) == batch_labels).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "font-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
