{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import Image as HuggingFaceImage\n",
    "from linformer import Linformer\n",
    "from vit_pytorch.efficient import ViT\n",
    "import torch\n",
    "def get_tokenizer() -> CLIPTokenizer:\n",
    "    return CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n",
    "def prepare_data(tokenizer: CLIPTokenizer):\n",
    "    def add_prompt(example):\n",
    "        props = example['font_properties']\n",
    "        character = example['character']\n",
    "        split = character.split('_')\n",
    "        if len(split) > 1:\n",
    "            character = split[0] + 'case ' + split[1]\n",
    "        else:\n",
    "            character = split[0]\n",
    "        prompt = f\"a {props['font_serifs']} {character} with {props['width']} width {props['rounding']} corners {props['font_weight']} weight and {props['dynamics']} movement with characteristics that can be described by adjectives {example['font_characteristics']}\" \n",
    "        example['prompt'] = prompt\n",
    "        return example\n",
    "    def map_tokens(example):\n",
    "        prompt = example['prompt']\n",
    "        tokens = tokenizer.encode(prompt, padding='max_length', max_length=42)\n",
    "        example['tokens'] = tokens\n",
    "        return example\n",
    "    dataset = load_dataset('json', data_files={'train':'train-metadata.jsonl', 'test':'test-metadata.jsonl'})\n",
    "    \n",
    "    train_new_column = ['foo'] * len(dataset['train'])\n",
    "    dataset['train'] = dataset['train'].add_column('prompt', train_new_column)\n",
    "    dataset['train'] = dataset['train'].add_column('tokens', train_new_column)\n",
    "    dataset['train'] = dataset['train'].map(add_prompt)\n",
    "    dataset['train'] = dataset['train'].map(map_tokens)\n",
    "    dataset['train'] = dataset['train'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['train'] = dataset['train'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['train'] = dataset['train'].with_format('torch')\n",
    "    \n",
    "    test_new_column = ['bar'] * len(dataset['test'])\n",
    "    dataset['test'] = dataset['test'].add_column('prompt', test_new_column)\n",
    "    dataset['test'] = dataset['test'].add_column('tokens', test_new_column)\n",
    "    dataset['test'] = dataset['test'].map(add_prompt)\n",
    "    dataset['test'] = dataset['test'].map(map_tokens)\n",
    "    dataset['test'] = dataset['test'].remove_columns(['prompt', 'uniqueId', 'ttf_path', 'font_characteristics', 'font_properties', 'character', 'vit_label'])\n",
    "    dataset['test'] = dataset['test'].cast_column('image', HuggingFaceImage())\n",
    "    dataset['test'] = dataset['test'].with_format('torch')\n",
    "    return dataset\n",
    "def get_vit_model(image_size: int, patch_size: int, dim: int, depth: int, num_heads: int, k: int, device: str):\n",
    "    sequence_length = (image_size//patch_size)**2 + 1\n",
    "    # for 512x512px image with 32x32px patches: 16x16 + 1 CLS token\n",
    "    efficient_transformer = Linformer(\n",
    "        dim=dim,\n",
    "        seq_len=sequence_length,  \n",
    "        depth=depth,\n",
    "        heads=num_heads,\n",
    "        k=k\n",
    "    )\n",
    "    model = ViT(\n",
    "        dim=dim,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_classes=62,\n",
    "        transformer=efficient_transformer,\n",
    "        channels=1,\n",
    "    )\n",
    "    return model \n",
    "def get_vit(image_size, patch_size, vit_dim, vit_depth, vit_num_heads, k, device, vit_checkpoint_path):\n",
    "    vit = get_vit_model(image_size=image_size, \n",
    "                        patch_size=patch_size, \n",
    "                        dim=vit_dim, \n",
    "                        depth=vit_depth, \n",
    "                        num_heads=vit_num_heads, \n",
    "                        k=k, \n",
    "                        device=device)\n",
    "    vit_checkpoint = torch.load(vit_checkpoint_path)\n",
    "    if vit_checkpoint != None:\n",
    "        vit.load_state_dict(vit_checkpoint['model_state_dict'])\n",
    "        print('Loaded ViT model from checkpoint:', vit_checkpoint_path)\n",
    "    return vit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Michael Labarca/.cache/huggingface/datasets/json/default-c772ad4eb6d31de9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac595c6e48de402ba749361819c40819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-6c27256c2b0087be.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-53a3018202bd379b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-a1fe57a82fffa92f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-8f3e544481ecc8a1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 12090\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 208\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = dataset['train'].filter(lambda ex: len(ex['tokens']) != 28)\n",
    "unique = set()\n",
    "for ex in filtered:\n",
    "    if len(ex['tokens']) == 35:\n",
    "        print(ex['prompt'])\n",
    "    unique.add(len(ex['tokens']))\n",
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinformerLM(nn.Module):\n",
    "    def __init__(self, num_tokens, dim, seq_len, depth, k = 256, heads = 8, dim_head = None, one_kv_head = False, share_kv = False, reversible = False, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.linformer = Linformer(dim, seq_len, depth, k = k, heads = heads, dim_head = dim_head,\n",
    "                one_kv_head = one_kv_head, share_kv = share_kv, reversible = reversible, dropout = dropout)\n",
    "        # self.to_logits = nn.Linear(dim, num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_emb(torch.arange(x.shape[1], device=x.device)) + x\n",
    "        x = self.linformer(x)\n",
    "        # out = self.to_logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/Michael Labarca/.cache/huggingface/datasets/json/default-c772ad4eb6d31de9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbbbe8c0c044c39ba15ef95861cf7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-6c27256c2b0087be.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-53a3018202bd379b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-a1fe57a82fffa92f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Michael Labarca\\.cache\\huggingface\\datasets\\json\\default-c772ad4eb6d31de9\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-8f3e544481ecc8a1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 12090\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'tokens'],\n",
      "        num_rows: 208\n",
      "    })\n",
      "})\n",
      "Loaded ViT model from checkpoint: ./vit-checkpoints/model-epoch18.pt\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 257, 128])\n",
      "torch.Size([2, 42, 256]) torch.Size([2, 256, 128])\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Michael Labarca\\AppData\\Local\\Temp\\ipykernel_5776\\2130757916.py\", line 71, in <module>\n",
      "    loss = clip(batch_tokens, batch_imgs, return_loss=True)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\x_clip\\x_clip.py\", line 645, in forward\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\x_clip\\x_clip.py\", line 395, in model_forward_with_context\n",
      "    enc = fn(*args)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\linformer\\linformer.py\", line 163, in forward\n",
      "    x = self.linformer(x)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\linformer\\linformer.py\", line 149, in forward\n",
      "    return self.net(x)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\linformer\\reversible.py\", line 149, in forward\n",
      "    x = x + f(x, **f_args)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\linformer\\linformer.py\", line 35, in forward\n",
      "    return self.fn(x)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\linformer\\linformer.py\", line 126, in forward\n",
      "    out = torch.einsum('bhnk,bhkd->bhnd', attn, values)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\torch\\functional.py\", line 378, in einsum\n",
      "    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\Michael Labarca\\.conda\\envs\\font-diffusion\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from x_clip import CLIP\n",
    "from vit_pytorch.extractor import Extractor\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "device = 'cuda'\n",
    "clip_tokenizer = get_tokenizer()\n",
    "dataset = prepare_data(clip_tokenizer)\n",
    "print(dataset)\n",
    "def get_dataloaders(train_clip_dataset, test_clip_dataset, batch_size):\n",
    "    train_loader = DataLoader(dataset=train_clip_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_clip_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "train_loader, valid_loader = get_dataloaders(train_dataset, test_dataset, 2)\n",
    "vit_checkpoint = './vit-checkpoints/model-epoch18.pt'\n",
    "image_size = 512\n",
    "patch_size = 32\n",
    "vit_dim = 128\n",
    "vit_depth = 12\n",
    "vit_num_heads = 8\n",
    "k = 64\n",
    "base_vit = get_vit(image_size, \n",
    "                    patch_size, \n",
    "                    vit_dim, \n",
    "                    vit_depth, \n",
    "                    vit_num_heads, \n",
    "                    k, \n",
    "                    device, \n",
    "                    vit_checkpoint)\n",
    "image_encoder = Extractor(\n",
    "    base_vit,\n",
    "    return_embeddings_only = True\n",
    ")\n",
    "text_encoder = LinformerLM(\n",
    "    num_tokens=49408,\n",
    "    dim = 256,\n",
    "    seq_len = 42,\n",
    "    depth = 12,\n",
    "    heads = 8,\n",
    "    dim_head = 64,        # be able to set the dimension of each head in multi-head attention\n",
    "    k = 128,               # this is the k that the key/values are projected to along the sequence dimension\n",
    "    one_kv_head = True,    # share one key/value head across all heads\n",
    "    share_kv = False,      # share the same projection for keys and values\n",
    "    reversible = False,      # make network reversible, like Reformer\n",
    ")\n",
    "clip = CLIP(\n",
    "    image_encoder = image_encoder,\n",
    "    text_encoder = text_encoder,\n",
    "    dim_image=128,\n",
    "    dim_text=256,\n",
    "    dim_latent=128,\n",
    "    text_encode_without_mask=True,\n",
    "    use_all_token_embeds=True,\n",
    "    text_has_cls_token=False,\n",
    "    visual_has_cls_token=True,\n",
    ").to(device)\n",
    "def prepare_batch(batch):\n",
    "    batch_imgs = batch['image']\n",
    "    batch_tokens = batch['tokens']\n",
    "    batch_imgs = batch_imgs[:, :, :, 0].unsqueeze(-1)\n",
    "    batch_imgs = batch_imgs.permute(0, 3, 1, 2)\n",
    "    batch_imgs = batch_imgs.type('torch.FloatTensor')\n",
    "    return batch_imgs, batch_tokens\n",
    "for batch in train_loader:\n",
    "    batch_imgs, batch_tokens = prepare_batch(batch)\n",
    "    # batch_imgs.to(device)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    batch_imgs = batch_imgs.to(device)\n",
    "    loss = clip(batch_tokens, batch_imgs, return_loss=True)\n",
    "    loss.backward()\n",
    "lr=3e-5\n",
    "def get_trainable_params(model):\n",
    "    return [params for params in model.parameters() if params.requires_grad]\n",
    "optimizer = AdamW(get_trainable_params(clip), lr=lr) # DALLE-pytorch setup\n",
    "for epoch in range(0, 10):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_imgs, batch_tokens = prepare_batch(batch)\n",
    "        # batch_imgs.to(device)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        loss = clip(batch_tokens, batch_imgs, return_loss=True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # acc = (output.argmax(dim=1) == batch_labels).float().mean()\n",
    "        # epoch_accuracy += acc / len(train_loader)\n",
    "        # epoch_loss += loss / len(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "font-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
